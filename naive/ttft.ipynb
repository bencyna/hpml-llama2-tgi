{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3e95c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccfd3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"YOUR_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cc9ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a28010",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France? Answer with only the city name.\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "max_new_tokens = 100\n",
    "current_input_ids = input_ids\n",
    "attention_mask = torch.ones_like(input_ids).long().to(\"cuda\")\n",
    "\n",
    "past_key_values = None\n",
    "start_time = None\n",
    "first_token_time = None\n",
    "output_ids = input_ids.clone()\n",
    "generated_tokens = 0\n",
    "\n",
    "GPU_COST_PER_HOUR = 2.93\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_new_tokens):\n",
    "\n",
    "        if i == 0:\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "\n",
    "        outputs = model(\n",
    "            current_input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        next_token_id = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        if i == 0:\n",
    "            torch.cuda.synchronize()\n",
    "            first_token_time = time.time()\n",
    "            ttft = first_token_time - start_time\n",
    "            print(f\"**Time to First Token (TTFT): {ttft:.4f} seconds**\")\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "\n",
    "        current_input_ids = next_token_id\n",
    "\n",
    "        new_attention_mask = torch.ones((1, 1), dtype=torch.long, device='cuda')\n",
    "        attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "\n",
    "        past_key_values = outputs.past_key_values\n",
    "        generated_tokens += 1\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "total_gen_time = end_time - first_token_time\n",
    "\n",
    "throughput = generated_tokens / total_gen_time if generated_tokens > 0 else 0\n",
    "overall_throughput = generated_tokens / (end_time - start_time)\n",
    "\n",
    "cost_per_second = GPU_COST_PER_HOUR / 3600\n",
    "cost_per_token = cost_per_second / throughput if throughput > 0 else float(\"inf\")\n",
    "sequence_cost = cost_per_token * generated_tokens\n",
    "\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Model Response ---\")\n",
    "print(response)\n",
    "print(f\"Total tokens generated: {generated_tokens}\")\n",
    "print(f\"Total generation time: {total_gen_time:.4f}s\")\n",
    "print(f\"Overall throughput (incl TTFT): {overall_throughput:.2f} tokens/s\")\n",
    "print(f\"Steady-state throughput: {throughput:.2f} tokens/s\")\n",
    "print(f\"Cost per token: ${cost_per_token:.8f}\")\n",
    "print(f\"Total sequence cost: ${sequence_cost:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307968fa",
   "metadata": {},
   "source": [
    "LATENCY BOUND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb970f38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def latency_bound_test(model, tokenizer, message, max_new_tokens=512):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = torch.ones_like(input_ids).long().to(\"cuda\")\n",
    "\n",
    "    current_input_ids = input_ids\n",
    "    past_key_values = None\n",
    "\n",
    "    output_ids = input_ids.clone()\n",
    "    generated_tokens = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ttft = None\n",
    "    start_time = None\n",
    "    first_token_time = None\n",
    "\n",
    "    STOP_TOKENS = {\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.pad_token_id,\n",
    "        tokenizer.bos_token_id,\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_new_tokens):\n",
    "\n",
    "            if i == 0:\n",
    "                torch.cuda.synchronize()\n",
    "                start_time = time.time()\n",
    "\n",
    "            outputs = model(\n",
    "                current_input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token_id = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            if i == 0:\n",
    "                torch.cuda.synchronize()\n",
    "                first_token_time = time.time()\n",
    "                ttft = first_token_time - start_time\n",
    "\n",
    "            if next_token_id.item() in STOP_TOKENS:\n",
    "                next_token_id = torch.tensor(\n",
    "                    [[tokenizer.encode(\"a\", add_special_tokens=False)[0]]],\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "            output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "            generated_tokens += 1\n",
    "\n",
    "            current_input_ids = next_token_id\n",
    "\n",
    "            new_attention_mask = torch.ones((1, 1), dtype=torch.long, device=\"cuda\")\n",
    "            attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_gen_time = end_time - first_token_time\n",
    "    throughput = generated_tokens / total_gen_time\n",
    "\n",
    "    return {\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"ttft\": ttft,\n",
    "        \"gen_time\": total_gen_time,\n",
    "        \"throughput\": throughput,\n",
    "        \"output\": tokenizer.decode(output_ids[0], skip_special_tokens=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44d169",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sizes = [256, 512, 1024]\n",
    "\n",
    "messages = [\n",
    "    {\"name\": \"simple_qa\", \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France? Answer with only the city name.\"}\n",
    "    ]},\n",
    "    {\"name\": \"reasoning\", \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Let's think step-by-step. If John is taller than Mark, and Mark is shorter than Sue, is John definitely taller than Sue? Answer 'Yes', 'No', or 'Cannot determine'.\"}\n",
    "    ]},\n",
    "    {\"name\": \"sentiment_analysis\", \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Classify the sentiment of the text as 'Positive', 'Negative', or 'Neutral'. Text: The service was quick and the food was delicious. Sentiment: Positive. Text: The package arrived late and the box was damaged. Sentiment: Negative. Text: The meeting ended on time. Sentiment: Neutral. Text: I finished the book but found the ending disappointing.Sentiment: [FILL IN HERE]\"}\n",
    "    ]},\n",
    "    {\"name\": \"summarization\", \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"You are an expert summarizer. Your goal is to write a single-paragraph, abstractive summary of the provided text, focusing on the main argument and conclusion. The summary must be brief, no more than 75 words. Use this article: https://en.wikipedia.org/wiki/Graphics_processing_unit\"}\n",
    "    ]},\n",
    "]\n",
    "\n",
    "for seq_len in sizes:\n",
    "  for message in messages:\n",
    "    print(f\"Testing {seq_len} Tokens On Prompt {message[\"name\"]}\")\n",
    "    r = latency_bound_test(model, tokenizer, message[\"messages\"], max_new_tokens=seq_len)\n",
    "\n",
    "    print(f\"Generated: {r['generated_tokens']} Tokens\")\n",
    "    print(f\"TTFT: {r['ttft']:.4f}s\")\n",
    "    print(f\"Generation Time: {r['gen_time']:.4f}s\")\n",
    "    print(f\"Throughput: {r['throughput']:.2f} tokens/sec\")\n",
    "    print(f\"Output: {r['output']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
