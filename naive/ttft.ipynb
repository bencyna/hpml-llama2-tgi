{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3e95c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccfd3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"YOUR_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cc9ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b75eb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, wandb\n",
    "wandb.init(project=\"hpml-final-project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a28010",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"hpml-final-project\",\n",
    "    group=\"summarization\",\n",
    "    name=f\"run5\",\n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You are an expert summarizer. Your goal is to write a single-paragraph, abstractive summary of the provided text, focusing on the main argument and conclusion. The summary must be brief, no more than 75 words. Use this article: https://en.wikipedia.org/wiki/Graphics_processing_unit.\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "max_new_tokens = 100\n",
    "current_input_ids = input_ids\n",
    "attention_mask = torch.ones_like(input_ids).long().to(\"cuda\")\n",
    "\n",
    "past_key_values = None\n",
    "start_time = None\n",
    "first_token_time = None\n",
    "output_ids = input_ids.clone()\n",
    "generated_tokens = 0\n",
    "\n",
    "ttft = 0\n",
    "\n",
    "GPU_COST_PER_HOUR = 2.93\n",
    "\n",
    "kv_cache_bytes_list = []\n",
    "kv_cache_mib_list = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def calculate_tensor_size(tensor):\n",
    "    if tensor is None:\n",
    "        return 0\n",
    "    return tensor.element_size() * tensor.numel()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_new_tokens):\n",
    "\n",
    "        if i == 0:\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "\n",
    "        outputs = model(\n",
    "            current_input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "\n",
    "        next_token_id = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "        total_kv_cache_bytes = 0\n",
    "\n",
    "        if outputs.past_key_values is not None:\n",
    "            for layer_cache in outputs.past_key_values:\n",
    "                if isinstance(layer_cache, tuple) and len(layer_cache) > 0:\n",
    "                    key_tensor, value_tensor = layer_cache[0], layer_cache[1]\n",
    "\n",
    "                    total_kv_cache_bytes += calculate_tensor_size(key_tensor)\n",
    "                    total_kv_cache_bytes += calculate_tensor_size(value_tensor)\n",
    "\n",
    "        total_kv_cache_mib = total_kv_cache_bytes / (1024 * 1024)\n",
    "\n",
    "        kv_cache_bytes_list.append(total_kv_cache_bytes)\n",
    "        kv_cache_mib_list.append(total_kv_cache_mib)\n",
    "\n",
    "        if i == 0:\n",
    "            torch.cuda.synchronize()\n",
    "            first_token_time = time.time()\n",
    "            ttft = first_token_time - start_time\n",
    "            print(f\"**Time to First Token (TTFT): {ttft:.4f} seconds**\")\n",
    "\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "\n",
    "        current_input_ids = next_token_id\n",
    "\n",
    "        new_attention_mask = torch.ones((1, 1), dtype=torch.long, device='cuda')\n",
    "        attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "\n",
    "        past_key_values = outputs.past_key_values\n",
    "        generated_tokens += 1\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "total_gen_time = end_time - first_token_time\n",
    "\n",
    "throughput = generated_tokens / total_gen_time if generated_tokens > 0 else 0\n",
    "overall_throughput = generated_tokens / (end_time - start_time)\n",
    "\n",
    "cost_per_second = GPU_COST_PER_HOUR / 3600\n",
    "cost_per_token = cost_per_second / throughput if throughput > 0 else float(\"inf\")\n",
    "sequence_cost = cost_per_token * generated_tokens\n",
    "\n",
    "max_kv_cache_mib = max(kv_cache_mib_list) if kv_cache_mib_list else 0\n",
    "\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n--- Model Response ---\")\n",
    "print(response)\n",
    "print(f\"Total tokens generated: {generated_tokens}\")\n",
    "print(f\"Total generation time: {total_gen_time:.4f}s\")\n",
    "print(f\"Overall throughput (incl TTFT): {overall_throughput:.2f} tokens/s\")\n",
    "print(f\"Steady-state throughput: {throughput:.2f} tokens/s\")\n",
    "print(f\"Cost per token: ${cost_per_token:.8f}\")\n",
    "print(f\"Max KV Cache Size: {max_kv_cache_mib:.2f} MiB\")\n",
    "print(f\"Total sequence cost: ${sequence_cost:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4b6fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_kv_cache_growth(kv_cache_mib_list, max_new_tokens, run_name):\n",
    "    x_axis = list(range(1, len(kv_cache_mib_list) + 1))\n",
    "    y_axis = kv_cache_mib_list\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(x_axis, y_axis, marker='o', linestyle='-', color='teal', markersize=4)\n",
    "    plt.xlabel('Token Position (Index)')\n",
    "    plt.ylabel('KV Cache Size (MiB)')\n",
    "    plt.title(f'KV Cache Growth during Generation ({run_name})')\n",
    "\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    max_mib = max(y_axis)\n",
    "    plt.annotate(\n",
    "        f'Max: {max_mib:.2f} MiB',\n",
    "        xy=(len(x_axis), max_mib),\n",
    "        xytext=(-50, 10),\n",
    "        textcoords='offset points',\n",
    "        arrowprops=dict(facecolor='black', shrink=0.05, width=1)\n",
    "    )\n",
    "\n",
    "    plot_filename = f\"kv_cache_growth_{run_name}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"KV Cache utilization plot saved to {plot_filename}\")\n",
    "    return plot_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f56ee4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "run_name = run.name\n",
    "\n",
    "plot_file = plot_kv_cache_growth(kv_cache_mib_list, max_new_tokens, run_name)\n",
    "\n",
    "wandb.log({\n",
    "    \"throughput\": throughput,\n",
    "    \"cost_per_token\": cost_per_token,\n",
    "    \"ttft\": ttft,\n",
    "    \"max_kv_cache_mib\": max_kv_cache_mib,\n",
    "    \"kv_cache_growth_plot\": wandb.Image(plot_file)\n",
    "})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307968fa",
   "metadata": {},
   "source": [
    "LATENCY BOUND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb970f38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_tensor_size(tensor: torch.Tensor) -> int:\n",
    "    if tensor is None:\n",
    "        return 0\n",
    "    return tensor.nelement() * tensor.element_size()\n",
    "\n",
    "def calculate_kv_cache_mib(past_key_values: tuple) -> float:\n",
    "    total_bytes = 0\n",
    "\n",
    "    if past_key_values is not None:\n",
    "        for layer_cache in past_key_values:\n",
    "            if isinstance(layer_cache, tuple) and len(layer_cache) >= 2:\n",
    "                key_tensor = layer_cache[0]\n",
    "                value_tensor = layer_cache[1]\n",
    "\n",
    "                total_bytes += calculate_tensor_size(key_tensor)\n",
    "                total_bytes += calculate_tensor_size(value_tensor)\n",
    "\n",
    "    return total_bytes / (1024 * 1024)\n",
    "\n",
    "def latency_bound_test(model, tokenizer, message, max_new_tokens=512):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = torch.ones_like(input_ids).long().to(\"cuda\")\n",
    "\n",
    "    current_input_ids = input_ids\n",
    "    past_key_values = None\n",
    "\n",
    "    output_ids = input_ids.clone()\n",
    "    generated_tokens = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    ttft = None\n",
    "    start_time = None\n",
    "    first_token_time = None\n",
    "\n",
    "    kv_cache_mib_list = []\n",
    "\n",
    "    STOP_TOKENS = {\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.pad_token_id,\n",
    "        tokenizer.bos_token_id,\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_new_tokens):\n",
    "\n",
    "            if i == 0:\n",
    "                torch.cuda.synchronize()\n",
    "                start_time = time.time()\n",
    "\n",
    "            outputs = model(\n",
    "                current_input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                attention_mask=attention_mask,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "            kv_cache_mib = calculate_kv_cache_mib(outputs.past_key_values)\n",
    "            kv_cache_mib_list.append(kv_cache_mib)\n",
    "\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token_id = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            if i == 0:\n",
    "                torch.cuda.synchronize()\n",
    "                first_token_time = time.time()\n",
    "                ttft = first_token_time - start_time\n",
    "\n",
    "            if next_token_id.item() in STOP_TOKENS:\n",
    "                next_token_id = torch.tensor(\n",
    "                    [[tokenizer.encode(\"a\", add_special_tokens=False)[0]]],\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "            output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "            generated_tokens += 1\n",
    "\n",
    "            current_input_ids = next_token_id\n",
    "\n",
    "            new_attention_mask = torch.ones((1, 1), dtype=torch.long, device=\"cuda\")\n",
    "            attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_gen_time = end_time - first_token_time\n",
    "    throughput = generated_tokens / total_gen_time\n",
    "\n",
    "    return {\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"ttft\": ttft,\n",
    "        \"gen_time\": total_gen_time,\n",
    "        \"throughput\": throughput,\n",
    "        \"output\": tokenizer.decode(output_ids[0], skip_special_tokens=True),\n",
    "        \"kv_cache_mib_list\": kv_cache_mib_list,\n",
    "        \"max_kv_cache_mib\": max(kv_cache_mib_list) if kv_cache_mib_list else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db44d169",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # {\"name\": \"simple_qa\", \"messages\": [\n",
    "    #     {\"role\": \"user\", \"content\": \"What is the capital of Australia? Answer with only the city name.\"}\n",
    "    # ]},\n",
    "    # {\"name\": \"reasoning\", \"messages\": [\n",
    "    #     {\"role\": \"user\", \"content\": \"Let's think step-by-step. If John is taller than Mark, and Mark is shorter than Sue, is John definitely taller than Sue? Answer 'Yes', 'No', or 'Cannot determine'.\"}\n",
    "    # ]},\n",
    "    # {\"name\": \"sentiment_analysis\", \"messages\": [\n",
    "    #     {\"role\": \"user\", \"content\": \"Classify the sentiment of the text as 'Positive', 'Negative', or 'Neutral'. Text: The service was quick and the food was delicious. Sentiment: Positive. Text: The package arrived late and the box was damaged. Sentiment: Negative. Text: The meeting ended on time. Sentiment: Neutral. Text: I finished the book but found the ending disappointing.Sentiment: [FILL IN HERE]\"}\n",
    "    # ]},\n",
    "    {\"name\": \"summarization\", \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"You are an expert summarizer. Your goal is to write a single-paragraph, abstractive summary of the provided text, focusing on the main argument and conclusion. The summary must be brief, no more than 75 words. Use this article: https://en.wikipedia.org/wiki/Graphics_processing_unit\"}\n",
    "    ]},\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "  run = wandb.init(\n",
    "      project=\"hpml-final-project\",\n",
    "      group=\"summarization\",\n",
    "      name=f\"latency_run3\",\n",
    "      reinit=True\n",
    "  )\n",
    "\n",
    "  r = latency_bound_test(model, tokenizer, message[\"messages\"], max_new_tokens=256)\n",
    "\n",
    "  print(f\"Generated: {r['generated_tokens']} Tokens\")\n",
    "  print(f\"TTFT: {r['ttft']:.4f}s\")\n",
    "  print(f\"Generation Time: {r['gen_time']:.4f}s\")\n",
    "  print(f\"Throughput: {r['throughput']:.2f} tokens/sec\")\n",
    "  print(f\"Max KV Cache Utilization: {r['max_kv_cache_mib']} MiB\")\n",
    "  print(f\"Output: {r['output']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf50fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "run_name = run.name\n",
    "\n",
    "plot_file = plot_kv_cache_growth(r['kv_cache_mib_list'], 256, run_name)\n",
    "\n",
    "cost_per_token = cost_per_second / r['throughput'] if throughput > 0 else float(\"inf\")\n",
    "\n",
    "wandb.log({\n",
    "    \"throughput\": r['throughput'],\n",
    "    \"cost_per_token\": cost_per_token,\n",
    "    \"ttft\": r['ttft'],\n",
    "    \"max_kv_cache_mib\": r['max_kv_cache_mib'],\n",
    "    \"kv_cache_growth_plot\": wandb.Image(plot_file)\n",
    "})\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5c959",
   "metadata": {},
   "source": [
    "THROUGHPUT-BOUND TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca80ff9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizer\n",
    "from typing import List\n",
    "\n",
    "def prepare_batched_inputs(tokenizer, messages: List[List[dict]]):\n",
    "    prompts = [tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def throughput_bound_test(model, tokenizer: PreTrainedTokenizer, messages: List[List[dict]], max_new_tokens: int = 256):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    inputs = prepare_batched_inputs(tokenizer, messages)\n",
    "    batch_size = inputs['input_ids'].shape[0]\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    total_tokens = 0\n",
    "    input_lengths = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # since the output contains the input tokens, we have to subtract them here\n",
    "    for i in range(batch_size):\n",
    "        total_tokens += output_ids.shape[1] - input_lengths\n",
    "\n",
    "    throughput = total_tokens / total_gen_time\n",
    "\n",
    "    return {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"throughput_tps\": throughput,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9134003",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "prompt = [\n",
    "    [{\"role\": \"user\", \"content\": \"You are an expert summarizer. Your goal is to write a single-paragraph, abstractive summary of the provided text, focusing on the main argument and conclusion. The summary must be brief, no more than 75 words. Use this article: https://en.wikipedia.org/wiki/Graphics_processing_unit\"}]\n",
    "]\n",
    "\n",
    "def test_throughput(config=None):\n",
    "    with wandb.init(config=config) as run:\n",
    "        config = run.config\n",
    "        current_batch_size = config.batch_size\n",
    "        max_new_tokens = config.max_new_tokens\n",
    "\n",
    "        total_messages = prompt * current_batch_size\n",
    "\n",
    "        result = throughput_bound_test(model, tokenizer, total_messages, max_new_tokens=max_new_tokens)\n",
    "\n",
    "        wandb.log({\n",
    "            \"batch_size\": result['batch_size'],\n",
    "            \"throughput\": result['throughput_tps'],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f853a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'name': 'Llama2-7B-Throughput-Bound-Summarization',\n",
    "    'parameters': {\n",
    "        'batch_size': {'values': [2, 4, 8, 16, 32, 64, 128, 256, 512]},\n",
    "        'max_new_tokens': {'value': 256}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"hpml-final-project\")\n",
    "wandb.agent(sweep_id, test_throughput)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
