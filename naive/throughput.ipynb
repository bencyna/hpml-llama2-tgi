{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3e95c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccfd3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"YOUR_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cc9ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a28010",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "prompt = \"The secret to happiness is\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "max_new_tokens = 100\n",
    "current_input_ids = input_ids\n",
    "attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to('cuda')\n",
    "past_key_values = None\n",
    "start_time = None\n",
    "first_token_time = None\n",
    "output_ids = input_ids.clone()\n",
    "generated_tokens = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "        \n",
    "        outputs = model(\n",
    "            current_input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        next_token_id = torch.argmax(logits, dim=-1).unsqueeze(-1)\n",
    "        \n",
    "        if i == 0:\n",
    "            torch.cuda.synchronize()\n",
    "            first_token_time = time.time()\n",
    "            ttft = first_token_time - start_time\n",
    "            print(f\"**Time to First Token (TTFT): {ttft:.4f} seconds**\")\n",
    "        \n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        output_ids = torch.cat([output_ids, next_token_id], dim=-1)\n",
    "        \n",
    "        current_input_ids = next_token_id\n",
    "        \n",
    "        new_attention_mask = torch.ones((1, 1), dtype=torch.long, device='cuda')\n",
    "        attention_mask = torch.cat([attention_mask, new_attention_mask], dim=1)\n",
    "        \n",
    "        past_key_values = outputs.past_key_values\n",
    "        generated_tokens += 1\n",
    "\n",
    "response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"\\n--- Model Response ---\")\n",
    "print(response)\n",
    "print(f\"Total tokens generated (excluding prompt): {generated_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
